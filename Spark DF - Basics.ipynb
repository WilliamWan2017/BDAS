{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DF - Basics\n",
    "\n",
    "Let's start off with the fundamentals of Spark DataFrame. The functionality in this tutorial has been adapted from \n",
    "Chang Hsin Lee. Find out more here:\n",
    "- https://changhsinlee.com/pyspark-dataframe-basics/\n",
    "\n",
    "Objective: In this exercise, you'll understand more about DataFrames, how to start a spark session, and carry out some basic data exploration, manipulation and aggregation. \n",
    "\n",
    "What is a DataFrame? A DataFrame is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. Find more information here: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "What is a Spark Session? It provides a single point of entry to interact with Spark's underlying functionality, which allows us to simply program Spark with DataFrame/Dataset APIs. A new Spark Session must be started in each of our notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. If you open the dataset, you'll find that each column has a header. We specify that by stating that header=True.\n",
    "# To make our lives easier, we can also use 'inferSchema' when importing CSVs. This automatically detects data types.\n",
    "# If you would like to manually change data types, refer to this article: https://medium.com/@mrpowers/adding-structtype-columns-to-spark-dataframes-b44125409803\n",
    "df = spark.read.csv(\"bank-additional-full.csv\",inferSchema=True,header=True,sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration\n",
    "Now that we've started the session and imported the data, let's explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "|age|        job| marital|          education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "| 56|  housemaid| married|           basic.4y|     no|     no|  no|telephone|  may|        mon|     261|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 57|   services| married|        high.school|unknown|     no|  no|telephone|  may|        mon|     149|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 37|   services| married|        high.school|     no|    yes|  no|telephone|  may|        mon|     226|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 40|     admin.| married|           basic.6y|     no|     no|  no|telephone|  may|        mon|     151|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 56|   services| married|        high.school|     no|     no| yes|telephone|  may|        mon|     307|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 45|   services| married|           basic.9y|unknown|     no|  no|telephone|  may|        mon|     198|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 59|     admin.| married|professional.course|     no|     no|  no|telephone|  may|        mon|     139|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|  may|        mon|     217|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 24| technician|  single|professional.course|     no|    yes|  no|telephone|  may|        mon|     380|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 25|   services|  single|        high.school|     no|    yes|  no|telephone|  may|        mon|      50|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 41|blue-collar| married|            unknown|unknown|     no|  no|telephone|  may|        mon|      55|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 25|   services|  single|        high.school|     no|    yes|  no|telephone|  may|        mon|     222|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 29|blue-collar|  single|        high.school|     no|     no| yes|telephone|  may|        mon|     137|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 57|  housemaid|divorced|           basic.4y|     no|    yes|  no|telephone|  may|        mon|     293|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 35|blue-collar| married|           basic.6y|     no|    yes|  no|telephone|  may|        mon|     146|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 54|    retired| married|           basic.9y|unknown|    yes| yes|telephone|  may|        mon|     174|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 35|blue-collar| married|           basic.6y|     no|    yes|  no|telephone|  may|        mon|     312|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 46|blue-collar| married|           basic.6y|unknown|    yes| yes|telephone|  may|        mon|     440|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 50|blue-collar| married|           basic.9y|     no|    yes| yes|telephone|  may|        mon|     353|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "| 39| management|  single|           basic.9y|unknown|     no|  no|telephone|  may|        mon|     195|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.857|     5191.0| no|\n",
      "+---+-----------+--------+-------------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The show method allows you visualise DataFrames in a tabular format. \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was originally from Kaggle (https://www.kaggle.com/rouseguy/bankbalanced/data). It's used to predict whether or not a client will subscribe to a term deposit (deposit column) if called by the banks call centre reps. You'll be using a simplified version of the original dataset throughout the DataFrame tutorials, and the full dataset in the binomial logistic regression machine learning exercise too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp.var.rate: double (nullable = true)\n",
      " |-- cons.price.idx: double (nullable = true)\n",
      " |-- cons.conf.idx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nr.employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n",
      "[Row(age=56, job='housemaid', marital='married', education='basic.4y', default='no', housing='no', loan='no', contact='telephone', month='may', day_of_week='mon', duration=261, campaign=1, pdays=999, previous=0, poutcome='nonexistent', emp.var.rate=1.1, cons.price.idx=93.994, cons.conf.idx=-36.4, euribor3m=4.857, nr.employed=5191.0, y='no')]\n"
     ]
    }
   ],
   "source": [
    "# Print schema allows us to visualise the data structure at a high level. \n",
    "df.printSchema()\n",
    "\n",
    "# We can also use head to print a specific amount of rows, so we can get a better understanding of the data points. \n",
    "# Note that we have to specify 'print' depending on the method we're using. Otherwise it may not show up!\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o98.describe.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:347)\n\tat scala.None$.get(Option.scala:345)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$aggregatableColumns$2.apply(Dataset.scala:235)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$aggregatableColumns$2.apply(Dataset.scala:233)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$aggregatableColumns(Dataset.scala:233)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2093)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2082)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2845)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2082)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-18ca95571664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We can use the describe method get some general statistics on our data too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.describe.\n: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:347)\n\tat scala.None$.get(Option.scala:345)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$aggregatableColumns$2.apply(Dataset.scala:235)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$aggregatableColumns$2.apply(Dataset.scala:233)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$aggregatableColumns(Dataset.scala:233)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2093)\n\tat org.apache.spark.sql.Dataset$$anonfun$describe$1.apply(Dataset.scala:2082)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2845)\n\tat org.apache.spark.sql.Dataset.describe(Dataset.scala:2082)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# We can use the describe method get some general statistics on our data too. \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, you may realise that we should have excluded the non-integer columns. But there is one interesting fact about this table. Martial has a count of 95 and Balance has a count of 96, while the others have a count of 100. Looks like there may be some missing data. We'll handle this in the upcoming data cleaning exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+\n",
      "|summary|              age|           balance|          duration|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "|  count|              100|                96|               100|\n",
      "|   mean|            40.92|1074.5520833333333|            931.97|\n",
      "| stddev|9.704398669436122|1709.7039686497387|353.62730638085895|\n",
      "|    min|               23|              -416|               395|\n",
      "|    max|               60|             10576|              2087|\n",
      "+-------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select the columns that are integers, and use the describe method again.\n",
    "# We see that the average age is 41. The average bank account balance is $1,074. \n",
    "# And they spoke to call centre reps for approx. 931 seconds on average. \n",
    "df.select('age', 'balance', 'duration').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Manipulation\n",
    "The code above shows you how to simply select columns, but there's much more that PySpark can do! Let's dig deeper into data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|balance|\n",
      "+-------+\n",
      "|   2343|\n",
      "|   null|\n",
      "|   1270|\n",
      "|   null|\n",
      "|    184|\n",
      "|      0|\n",
      "|    830|\n",
      "|    545|\n",
      "|      1|\n",
      "|   null|\n",
      "|    100|\n",
      "|    309|\n",
      "|    199|\n",
      "|    460|\n",
      "|    703|\n",
      "|   3837|\n",
      "|   null|\n",
      "|     -8|\n",
      "|     55|\n",
      "|    168|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select the balance column and assign it to a variable. \n",
    "bal_col = df.select('balance')\n",
    "\n",
    "# We can then use the show method on that variable.\n",
    "bal_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+----------------+\n",
      "|age|        job| marital|education|balance|housing|loan|duration|deposit|balance_times_10|\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+----------------+\n",
      "| 59|     admin.|    null|secondary|   2343|    yes|  no|    1042|    yes|           23430|\n",
      "| 56|     admin.| married|secondary|   null|     no|  no|    1467|    yes|            null|\n",
      "| 41| technician| married|secondary|   1270|    yes|  no|    1389|    yes|           12700|\n",
      "| 55|   services|    null|secondary|   null|    yes|  no|     579|    yes|            null|\n",
      "| 54|     admin.| married| tertiary|    184|     no|  no|     673|    yes|            1840|\n",
      "| 42| management|    null| tertiary|      0|    yes| yes|     562|    yes|               0|\n",
      "| 56| management| married| tertiary|    830|    yes| yes|    1201|    yes|            8300|\n",
      "| 60|    retired|    null|secondary|    545|    yes|  no|    1030|    yes|            5450|\n",
      "| 37| technician| married|secondary|      1|    yes|  no|     608|    yes|              10|\n",
      "| 28|   services|    null|secondary|   null|    yes|  no|    1297|    yes|            null|\n",
      "| 38|     admin.|  single|secondary|    100|    yes|  no|     786|    yes|            1000|\n",
      "| 30|blue-collar| married|secondary|    309|    yes|  no|    1574|    yes|            3090|\n",
      "| 29| management| married| tertiary|    199|    yes| yes|    1689|    yes|            1990|\n",
      "| 46|blue-collar|  single| tertiary|    460|    yes|  no|    1102|    yes|            4600|\n",
      "| 31| technician|  single| tertiary|    703|    yes|  no|     943|    yes|            7030|\n",
      "| 35| management|divorced| tertiary|   3837|    yes|  no|    1084|    yes|           38370|\n",
      "| 32|blue-collar|  single|  primary|   null|    yes|  no|     541|    yes|            null|\n",
      "| 49|   services| married|secondary|     -8|    yes|  no|    1119|    yes|             -80|\n",
      "| 41|     admin.| married|secondary|     55|    yes|  no|    1120|    yes|             550|\n",
      "| 49|     admin.|divorced|secondary|    168|    yes| yes|     513|    yes|            1680|\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+\n",
      "|age|        job| marital|education|balance|housing|loan|duration|deposit|\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+\n",
      "| 59|     admin.|    null|secondary|   2343|    yes|  no|    1042|    yes|\n",
      "| 56|     admin.| married|secondary|   null|     no|  no|    1467|    yes|\n",
      "| 41| technician| married|secondary|   1270|    yes|  no|    1389|    yes|\n",
      "| 55|   services|    null|secondary|   null|    yes|  no|     579|    yes|\n",
      "| 54|     admin.| married| tertiary|    184|     no|  no|     673|    yes|\n",
      "| 42| management|    null| tertiary|      0|    yes| yes|     562|    yes|\n",
      "| 56| management| married| tertiary|    830|    yes| yes|    1201|    yes|\n",
      "| 60|    retired|    null|secondary|    545|    yes|  no|    1030|    yes|\n",
      "| 37| technician| married|secondary|      1|    yes|  no|     608|    yes|\n",
      "| 28|   services|    null|secondary|   null|    yes|  no|    1297|    yes|\n",
      "| 38|     admin.|  single|secondary|    100|    yes|  no|     786|    yes|\n",
      "| 30|blue-collar| married|secondary|    309|    yes|  no|    1574|    yes|\n",
      "| 29| management| married| tertiary|    199|    yes| yes|    1689|    yes|\n",
      "| 46|blue-collar|  single| tertiary|    460|    yes|  no|    1102|    yes|\n",
      "| 31| technician|  single| tertiary|    703|    yes|  no|     943|    yes|\n",
      "| 35| management|divorced| tertiary|   3837|    yes|  no|    1084|    yes|\n",
      "| 32|blue-collar|  single|  primary|   null|    yes|  no|     541|    yes|\n",
      "| 49|   services| married|secondary|     -8|    yes|  no|    1119|    yes|\n",
      "| 41|     admin.| married|secondary|     55|    yes|  no|    1120|    yes|\n",
      "| 49|     admin.|divorced|secondary|    168|    yes| yes|     513|    yes|\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also add columns and manipulate the DataFrame. Let's times balance by 10, and add the output to a new column.\n",
    "df.withColumn('balance_times_10',df['balance']*10).show()\n",
    "\n",
    "# Question: If we print the df DataFrame again, why is the 'balance_times_10 column' missing?\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+\n",
      "|age|        job| marital|education|balance|housing|loan|duration|deposit|\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+\n",
      "| 51|blue-collar| married|secondary|   7180|    yes|  no|     927|    yes|\n",
      "| 41|blue-collar|divorced|secondary|   5291|    yes|  no|    1423|    yes|\n",
      "| 29| management| married| tertiary|  10576|     no|  no|    1224|    yes|\n",
      "+---+-----------+--------+---------+-------+-------+----+--------+-------+\n",
      "\n",
      "+-----------+-------+\n",
      "|        job|balance|\n",
      "+-----------+-------+\n",
      "| management|   3837|\n",
      "| technician|   3285|\n",
      "|  housemaid|   3923|\n",
      "|blue-collar|   2823|\n",
      "| technician|   3652|\n",
      "|blue-collar|   7180|\n",
      "|blue-collar|   5291|\n",
      "| technician|   4580|\n",
      "| management|  10576|\n",
      "| technician|   3706|\n",
      "| management|   4393|\n",
      "|blue-collar|   4438|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try out some additional DataFrame methods.\n",
    "# How would we identify individuals with a balance above $5,000? Using filter! \n",
    "df.filter(\"balance > 5000\").show()\n",
    "\n",
    "# We can also use more advanced filters. For example, let's see the jobs of people with over $2,500 in their bank account.\n",
    "df.filter(\"balance > 2500\").select('job','balance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------+\n",
      "|age|        job|balance|\n",
      "+---+-----------+-------+\n",
      "| 35| management|   3837|\n",
      "| 29| management|  10576|\n",
      "| 27| technician|   3706|\n",
      "| 36|blue-collar|   4438|\n",
      "+---+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What if we wanted to identify those that were under 40 and had over $2,500 in their account? \n",
    "# We can use multiple conditions.\n",
    "df.filter(\"balance > 2500 AND age < 40\").select('age','job','balance').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Aggregation\n",
    "On top of filtering, we can also group/aggregate data. Let's see how that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+-----------------+\n",
      "|          job|          avg(age)|      avg(balance)|    avg(duration)|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "|   management|           40.4375|         1771.5625|          1139.75|\n",
      "|      retired|              55.0|            843.75|          1060.25|\n",
      "|      unknown|              49.0|             341.0|            520.0|\n",
      "|self-employed|              31.0|             144.0|            676.0|\n",
      "|  blue-collar| 39.48571428571429|1050.6470588235295|917.9714285714285|\n",
      "| entrepreneur|              34.0|            -195.5|            432.5|\n",
      "|       admin.|              42.0| 524.1538461538462|            815.5|\n",
      "|   technician|39.411764705882355|1343.3529411764705|980.2941176470588|\n",
      "|     services|              43.5| 277.6666666666667|          815.125|\n",
      "|    housemaid|              52.0|            3923.0|            942.0|\n",
      "|   unemployed|              37.0|             381.0|            985.0|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('job').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened? Our dataset was grouped by job title (technician, management, etc.) and the average age, balance and duration for each job was calculated. Why only these three? Because mean() automatically filters out any non-numeric features. But in most cases, it's good practice to sort. Let's see how that's done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Age\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "|          job|          avg(age)|      avg(balance)|    avg(duration)|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "|self-employed|              31.0|             144.0|            676.0|\n",
      "| entrepreneur|              34.0|            -195.5|            432.5|\n",
      "|   unemployed|              37.0|             381.0|            985.0|\n",
      "|   technician|39.411764705882355|1343.3529411764705|980.2941176470588|\n",
      "|  blue-collar| 39.48571428571429|1050.6470588235295|917.9714285714285|\n",
      "|   management|           40.4375|         1771.5625|          1139.75|\n",
      "|       admin.|              42.0| 524.1538461538462|            815.5|\n",
      "|     services|              43.5| 277.6666666666667|          815.125|\n",
      "|      unknown|              49.0|             341.0|            520.0|\n",
      "|    housemaid|              52.0|            3923.0|            942.0|\n",
      "|      retired|              55.0|            843.75|          1060.25|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "\n",
      "Sorted by Balance\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "|          job|          avg(age)|      avg(balance)|    avg(duration)|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "| entrepreneur|              34.0|            -195.5|            432.5|\n",
      "|self-employed|              31.0|             144.0|            676.0|\n",
      "|     services|              43.5| 277.6666666666667|          815.125|\n",
      "|      unknown|              49.0|             341.0|            520.0|\n",
      "|   unemployed|              37.0|             381.0|            985.0|\n",
      "|       admin.|              42.0| 524.1538461538462|            815.5|\n",
      "|      retired|              55.0|            843.75|          1060.25|\n",
      "|  blue-collar| 39.48571428571429|1050.6470588235295|917.9714285714285|\n",
      "|   technician|39.411764705882355|1343.3529411764705|980.2941176470588|\n",
      "|   management|           40.4375|         1771.5625|          1139.75|\n",
      "|    housemaid|              52.0|            3923.0|            942.0|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To simplify things, let's split this into two steps. First, let's create a variable then order by age.\n",
    "# Careful when using show()! Otherwise the variable type will change and you won't be able to order it. \n",
    "group_job_df = df.groupBy('job').mean()\n",
    "\n",
    "# Note that we have to use 'avg(age)' instead of age. Why? Because when you use mean(), it changes the feature's name (as you can see below).\n",
    "print(\"Sorted by Age\")\n",
    "group_job_df.orderBy('avg(age)').show()\n",
    "\n",
    "# Let's see what this looks like in one line.\n",
    "print(\"Sorted by Balance\")\n",
    "df.groupBy('job').mean().orderBy('avg(balance)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "While the data may be accurate, it's still not necessarily appropriate in a professional context. Let's make a few adjustments to make it more appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+-----------------+\n",
      "|          job|          avg(age)|      avg(balance)|    avg(duration)|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "|   management|           40.4375|         1771.5625|          1139.75|\n",
      "|      retired|              55.0|            843.75|          1060.25|\n",
      "|      unknown|              49.0|             341.0|            520.0|\n",
      "|self-employed|              31.0|             144.0|            676.0|\n",
      "|  blue-collar| 39.48571428571429|1050.6470588235295|917.9714285714285|\n",
      "| entrepreneur|              34.0|            -195.5|            432.5|\n",
      "|       admin.|              42.0| 524.1538461538462|            815.5|\n",
      "|   technician|39.411764705882355|1343.3529411764705|980.2941176470588|\n",
      "|     services|              43.5| 277.6666666666667|          815.125|\n",
      "|    housemaid|              52.0|            3923.0|            942.0|\n",
      "|   unemployed|              37.0|             381.0|            985.0|\n",
      "+-------------+------------------+------------------+-----------------+\n",
      "\n",
      "+-------------+--------------------------+------------------------------+-------------------------------+\n",
      "|          job|format_number(avg(age), 2)|format_number(avg(balance), 2)|format_number(avg(duration), 2)|\n",
      "+-------------+--------------------------+------------------------------+-------------------------------+\n",
      "|   management|                     40.44|                      1,771.56|                       1,139.75|\n",
      "|      retired|                     55.00|                        843.75|                       1,060.25|\n",
      "|      unknown|                     49.00|                        341.00|                         520.00|\n",
      "|self-employed|                     31.00|                        144.00|                         676.00|\n",
      "|  blue-collar|                     39.49|                      1,050.65|                         917.97|\n",
      "| entrepreneur|                     34.00|                       -195.50|                         432.50|\n",
      "|       admin.|                     42.00|                        524.15|                         815.50|\n",
      "|   technician|                     39.41|                      1,343.35|                         980.29|\n",
      "|     services|                     43.50|                        277.67|                         815.12|\n",
      "|    housemaid|                     52.00|                      3,923.00|                         942.00|\n",
      "|   unemployed|                     37.00|                        381.00|                         985.00|\n",
      "+-------------+--------------------------+------------------------------+-------------------------------+\n",
      "\n",
      "+-------------+-----------+---------------+----------------+\n",
      "| Job Category|Average Age|Average Balance|Average Duration|\n",
      "+-------------+-----------+---------------+----------------+\n",
      "|   management|      40.44|       1,771.56|        1,139.75|\n",
      "|      retired|      55.00|         843.75|        1,060.25|\n",
      "|      unknown|      49.00|         341.00|          520.00|\n",
      "|self-employed|      31.00|         144.00|          676.00|\n",
      "|  blue-collar|      39.49|       1,050.65|          917.97|\n",
      "| entrepreneur|      34.00|        -195.50|          432.50|\n",
      "|       admin.|      42.00|         524.15|          815.50|\n",
      "|   technician|      39.41|       1,343.35|          980.29|\n",
      "|     services|      43.50|         277.67|          815.12|\n",
      "|    housemaid|      52.00|       3,923.00|          942.00|\n",
      "|   unemployed|      37.00|         381.00|          985.00|\n",
      "+-------------+-----------+---------------+----------------+\n",
      "\n",
      "Average Age, Balance and Duration by Job Category\n",
      "+-------------+-----------+---------------+----------------+\n",
      "| Job Category|Average Age|Average Balance|Average Duration|\n",
      "+-------------+-----------+---------------+----------------+\n",
      "|self-employed|      31.00|         144.00|          676.00|\n",
      "| entrepreneur|      34.00|        -195.50|          432.50|\n",
      "|   unemployed|      37.00|         381.00|          985.00|\n",
      "|   technician|      39.41|       1,343.35|          980.29|\n",
      "|  blue-collar|      39.49|       1,050.65|          917.97|\n",
      "|   management|      40.44|       1,771.56|        1,139.75|\n",
      "|       admin.|      42.00|         524.15|          815.50|\n",
      "|     services|      43.50|         277.67|          815.12|\n",
      "|      unknown|      49.00|         341.00|          520.00|\n",
      "|    housemaid|      52.00|       3,923.00|          942.00|\n",
      "|      retired|      55.00|         843.75|        1,060.25|\n",
      "+-------------+-----------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number, col\n",
    "\n",
    "# Let's start off with this. Just grouping by job and presenting the mean.\n",
    "group_job_df = df.groupBy('job').mean()\n",
    "group_job_df.show()\n",
    "\n",
    "# Now that we've calculated the mean, the values for blue-collar and technician are extremely long. \n",
    "# We can use format_number to reduce the total amount of decimals. \n",
    "# The number two represents the amount of decimals we want to be displayed.\n",
    "group_job_df = group_job_df.select('job',\n",
    "                                   format_number('avg(age)',2),\n",
    "                                   format_number('avg(balance)',2),\n",
    "                                   format_number('avg(duration)',2))\n",
    "group_job_df.show()\n",
    "\n",
    "# But now the column names look quite unprofessional. We can assign an alias to rename each of them.\n",
    "group_job_df = group_job_df.select(col('job').alias('Job Category'),\n",
    "                                   col('format_number(avg(age), 2)').alias('Average Age'),\n",
    "                                   col('format_number(avg(balance), 2)').alias('Average Balance'),\n",
    "                                   col('format_number(avg(duration), 2)').alias('Average Duration'))\n",
    "group_job_df.show()\n",
    "\n",
    "# Finally, let's sort the DataFrame by age.\n",
    "group_job_df = group_job_df.orderBy('Average Age')\n",
    "\n",
    "print('Average Age, Balance and Duration by Job Category')\n",
    "group_job_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great job on finishing! \n",
    "\n",
    "Let's go over a few additional key takeaways:\n",
    "- You should understand why group_job_df was reassigned each time. \n",
    "- Also, you should know that using pyspark.sql.functions is not the only way of achieving such tasks. You could use a different package, function or method (check out the documentation, or click here: https://stackoverflow.com/questions/34077353/how-to-change-dataframe-column-names-in-pyspark)\n",
    "- Finally, you should realise that the PySpark API allows you to fully utilise the Python programming language. You don't have to be explicit like in the code example above - that was for the sake of simplicity. If you're comfortable with programming, try using a loop to make repetitive work faster and simpler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
